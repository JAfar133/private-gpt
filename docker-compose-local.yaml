#don't forget to install models for ollama
#you can do this by entering bash of running docker ollama container and running the following commands there:
#ollama run mistral
#ollama run nomic-embed-text
services:
  private-gpt:
    build:
      dockerfile: Dockerfile.local
    volumes:
      - ./local_data/:/home/worker/app/local_data
    ports:
      - 8001:8080
    environment:
      PORT: 8080
      PGPT_PROFILES: docker
      PGPT_MODE: ollama
  ollama:
    image: ollama/ollama:latest
    volumes:
      - models:/root/.ollama
    ports:
      - 11434:11434
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           capabilities: [gpu]
volumes:
  models:
    driver: local